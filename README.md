Effect of PCA on Model Accuracy and Training Time

A comparative study on how Principal Component Analysis (PCA) impacts the performance and computational cost of popular Machine Learning models.

ğŸ§  Project Overview

This project analyzes the effect of dimensionality reduction using PCA on:

Model Accuracy

Training Time

Components vs Performance Trade-off

Three ML models were tested:

Logistic Regression

Support Vector Classifier (SVC)

Random Forest Classifier

The goal is to understand whether PCA improves performance, reduces overfitting, and speeds up training.

ğŸ—ï¸ Methodology
1ï¸âƒ£ Preprocessing

Standard Scaling (StandardScaler)

PCA on multiple component counts

Train-test split

Iterative experiments for consistency

2ï¸âƒ£ Models Evaluated

Logistic Regression

SVM (RBF)

Random Forest

3ï¸âƒ£ Metrics Used

Accuracy

Training Time

Variance Explained

Componentâ€“Performance Curves

4ï¸âƒ£ Visualizations

Accuracy vs Model

Training Time vs Components

Accuracy vs Training Time

PCA Variance Explained Curve

ğŸ“ˆ Key Results
âœ”ï¸ Logistic Regression

Best accuracy after PCA

PCA preserved linear separability

Training time significantly reduced

âœ”ï¸ SVM

Slight drop in accuracy

PCA improved training speed

Non-linear models lose performance with PCA

âœ”ï¸ Random Forest

Accuracy decreased

PCA didnâ€™t improve trees

Training time benefited slightly

â­ Overall Conclusion

PCA improves training speed for all models.
PCA preserves accuracy only for linear models.
Too many or too few components â†’ accuracy drops.
Optimal region â‰ˆ 95% variance retention.

ğŸ“Š Professional Plots
Accuracy Comparison

Shows how different models behave after PCA.

Training Time vs Components

Demonstrates how PCA drastically reduces training cost.

Accuracy vs Training Time

Helps visualize the trade-off between performance and speed.

PCA Explained Variance Curve

Shows the ideal number of components needed to retain information.

(Plots are generated via the code in the notebook.)

ğŸ“ Tech Stack

Python

Scikit-learn (PCA, ML models)

Matplotlib & Seaborn

NumPy / Pandas

ğŸ§ª How to Run
pip install numpy pandas scikit-learn matplotlib seaborn


Run the notebook:

jupyter notebook pca_analysis.ipynb

ğŸ“Œ Future Work

Compare PCA with t-SNE / UMAP

Try PCA before Deep Learning

Hyperparameter tuning with PCA

Analyze biasâ€“variance changes

ğŸ‘¤ Author

Aman Pal
B.S. Student, IIT Patna
Focus: Machine Learning, NLP Research
